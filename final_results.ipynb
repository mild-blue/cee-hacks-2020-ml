{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "data_location = os.path.normpath('../data')\n",
    "\n",
    "possible_data_files = [\"hackathon_low_mixed_venous_oximetry.csv\", \"hackathon_low_cardiac_output.csv\"]\n",
    "\n",
    "data_paths = [os.path.join(data_location, data_file) for data_file in possible_data_files]\n",
    "\n",
    "CHOSEN_TASK = 1\n",
    "data_path = data_paths[CHOSEN_TASK]\n",
    "\n",
    "\n",
    "test_data_files = [\"hackathon_low_mixed_venous_oximetry_test_set.csv\", \"hackathon_low_cardiac_output_test_set.csv\"]\n",
    "\n",
    "df_test_new = os.path.join(data_location, test_data_files[CHOSEN_TASK])\n",
    "df_test_new = pd.read_csv(df_test_new)\n",
    "\n",
    "df_test_new = (\n",
    "    df_test_new\n",
    "    .assign(ClassificationLabel = lambda df: df.ClassificationLabel==\"Positive\")\n",
    "    .assign(gender=lambda df:df.gender==\"F\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = (\n",
    "    pd.read_csv(data_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom columns and names per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bsa(weight, height):\n",
    "    return np.sqrt(weight*height*100)/60\n",
    "    return pow(weight,0.425)*pow(height*100,0.725)*0.007184\n",
    "\n",
    "def fun(s):\n",
    "    return bsa(s.weight, s.height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "feature_columns = [col for col in df_raw.columns if col not in [\"event_count\",\n",
    "                                                            \"ClassificationLabel\",\n",
    "                                                           \"subject_id\"]  and \"Regression\" not in col]\n",
    "\n",
    "if 'cardiac_output' in data_path:\n",
    "    regression_label = \"RegressionLabel-CardiacIndex\"\n",
    "#     remove data with wrong BSA\n",
    "    df_raw = df_raw.loc[lambda df: (df.apply(lambda df: fun(df), axis = 1) - df.bsa) < 0.1]\n",
    "else:\n",
    "    regression_label = \"RegressionLabel-SvO2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows\")\n",
    "len(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows per patient\")\n",
    "df_raw.groupby(\"subject_id\").count().event_count.value_counts().sort_index().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of nans per row\")\n",
    "df_raw.isna().sum(axis=1).value_counts().sort_index().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of nans per column\")\n",
    "df_raw.isna().sum(axis=0).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop patients with suspicion of Pulmonary hypertension\n",
    "df.loc[lambda df: (df.Pulmonary_Artery_Mean_Pressure > 100) | (df.Pulmonary_Artery_Mean_Pressure == 0),\"Pulmonary_Artery_Mean_Pressure\"] = np.nan\n",
    "df.loc[lambda df: df.Central_Venous_Pressure > 100, \"Central_Venous_Pressure\"] = np.nan\n",
    "if \"End_Diastolic_Volume\" in df.columns:\n",
    "    df.loc[lambda df: df.End_Diastolic_Volume > 400, \"End_Diastolic_Volume\"] = np.nan\n",
    "df.loc[lambda df: df.Heart_Rate < 40, \"Heart_Rate\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    .assign(ClassificationLabel = lambda df: df.ClassificationLabel==\"Positive\")\n",
    "    .assign(gender=lambda df:df.gender==\"F\")\n",
    ")\n",
    "\n",
    "mean_columns = feature_columns+[\"subject_id\"]\n",
    "for col in mean_columns:\n",
    "    if df[col].dtypes in ['object', 'bool', 'str']:\n",
    "        mean_columns.remove(col)\n",
    "        \n",
    "means = df[mean_columns].groupby(\"subject_id\").mean()\n",
    "\n",
    "FILL_IN = \"overall_mean\"\n",
    "# FILL_IN = \"drop\"\n",
    "# FILL_IN = \"patient_mean\"\n",
    "# FILL_IN = \"keep\"\n",
    "\n",
    "if FILL_IN == \"patient_mean\":\n",
    "    to_drop = []\n",
    "    for row_id, row in df[mean_columns].iterrows():\n",
    "        for col, elem in row.items():\n",
    "            if pd.isna(elem):\n",
    "                new = means.loc[row.subject_id, col]\n",
    "                if pd.isna(new):\n",
    "                    to_drop.append(row_id)\n",
    "                else:    \n",
    "                    df.loc[row_id, col] = new\n",
    "\n",
    "    df = df.loc[lambda df: ~df.index.isin(to_drop)]\n",
    "    print(f\"Dropped {len(to_drop)} rows because we were unabble to fill them\")\n",
    "\n",
    "elif FILL_IN == \"overall_mean\": \n",
    "    for col in feature_columns:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "elif FILL_IN == \"drop\":\n",
    "    previous_len = len(df)\n",
    "    df = df.dropna()\n",
    "    print(f\"Dropped {previous_len - len(df)}\")\n",
    "    \n",
    "    \n",
    "means = means.add_suffix('___mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "subjects = list(set(df.subject_id))\n",
    "random.shuffle(subjects)\n",
    "N = int(len(subjects)/5*4)\n",
    "train_subjects = subjects[:N]\n",
    "test_subjects = subjects[N:]\n",
    "\n",
    "\n",
    "df_train=df.loc[lambda df: df.subject_id.isin(train_subjects)]\n",
    "df_test=df.loc[lambda df: df.subject_id.isin(test_subjects)]\n",
    "\n",
    "print(len(df_test))\n",
    "print(len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Model training High risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "threshold_training = 2.6\n",
    "threshold_validation_test = 2.4\n",
    "thresh = threshold_validation_test\n",
    "X = df_train[feature_columns].values\n",
    "y = df_train[regression_label]<threshold_training\n",
    "\n",
    "X_val = df_test[feature_columns].values\n",
    "y_val = df_test[regression_label]<threshold_validation_test\n",
    "\n",
    "\n",
    "X_test = df_test_new[feature_columns].values\n",
    "y_test = df_test_new[regression_label]<threshold_validation_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import xgboost\n",
    "# clf = tree.DecisionTreeClassifier(max_depth=2)\n",
    "clf = xgboost.XGBClassifier(scale_pos_weight=1,\n",
    "                            max_depth=2,\n",
    "                            subsample=1,\n",
    "                            colsample_bytree=1,\n",
    "                            min_child_weight=40\n",
    "                           )\n",
    "clf = clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassBalance, ROCAUC, ClassificationReport, ClassPredictionError\n",
    "\n",
    "clf.target_type_ = int\n",
    "rocauc = ROCAUC(clf, size=(700, 500), classes=[0,1])\n",
    "\n",
    "rocauc.score(X_test, y_test)  \n",
    "r = rocauc.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "25/35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.044\n",
    "y_test_predict = clf.predict_proba(X_test)[:,0] < THRESHOLD\n",
    "import sklearn.metrics as metrics\n",
    "conf_matrix = metrics.confusion_matrix(y_test, y_test_predict)\n",
    "conf_df = pd.DataFrame(conf_matrix,\n",
    "            index=[f\"label_{i}\" for i in range(0, max(y_test)+1)],\n",
    "            columns=[f\"pred_{i}\" for i in range(0, max(y_test)+1)])\n",
    "conf_df.loc[\"label_0\"] = 10*conf_df.loc[\"label_0\"]\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict_proba(df[feature_columns].values)[:,0] < THRESHOLD\n",
    "\n",
    "high_risk_patients = df.loc[predict]\n",
    "print(f\"Number of high risk in whole data {len(high_risk_patients)}\")\n",
    "print(f\"Number of mistakes {(high_risk_patients[regression_label]>thres).sum()*10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risk_patients_summary = suspicious_pacients[feature_columns].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Model training low risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "threshold_training = 2.7\n",
    "threshold_validation_test = 2.4\n",
    "thresh = threshold_validation_test\n",
    "X = df_train[feature_columns].values\n",
    "y = df_train[regression_label]<threshold_training\n",
    "\n",
    "X_val = df_test[feature_columns].values\n",
    "y_val = df_test[regression_label]<threshold_validation_test\n",
    "\n",
    "\n",
    "X_test = df_test_new[feature_columns].values\n",
    "y_test = df_test_new[regression_label]<threshold_validation_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import xgboost\n",
    "# clf = tree.DecisionTreeClassifier(max_depth=2)\n",
    "clf = xgboost.XGBClassifier(scale_pos_weight=1,\n",
    "                            max_depth=2,\n",
    "                            subsample=1,\n",
    "                            colsample_bytree=1,\n",
    "                            min_child_weight=40\n",
    "                           )\n",
    "clf = clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassBalance, ROCAUC, ClassificationReport, ClassPredictionError\n",
    "\n",
    "clf.target_type_ = int\n",
    "rocauc = ROCAUC(clf, size=(700, 500), classes=[0,1])\n",
    "\n",
    "rocauc.score(X_test, y_test)  \n",
    "r = rocauc.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.7\n",
    "y_test_predict = clf.predict_proba(X_test)[:,0] < THRESHOLD\n",
    "import sklearn.metrics as metrics\n",
    "conf_matrix = metrics.confusion_matrix(y_test, y_test_predict)\n",
    "conf_df = pd.DataFrame(conf_matrix,\n",
    "            index=[f\"label_{i}\" for i in range(0, max(y_test)+1)],\n",
    "            columns=[f\"pred_{i}\" for i in range(0, max(y_test)+1)])\n",
    "conf_df.loc[\"label_0\"] = 10*conf_df.loc[\"label_0\"]\n",
    "conf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict_proba(df[feature_columns].values)[:,0] > THRESHOLD\n",
    "suspicious_pacients = df.loc[predict]\n",
    "print(f\"number of identified {len(suspicious_pacients)*10}\")\n",
    "print(f\"Number of mistakes {(suspicious_pacients[regression_label]<thresh).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = suspicious_pacients[feature_columns].mean().to_frame(name=\"suspicuos_negative\")\n",
    "\n",
    "comparison[\"rest_negative\"] = df_test[feature_columns].loc[~predict & (df[regression_label]>thres)].mean()\n",
    "comparison[\"rest_positive\"] = df_test[feature_columns].loc[~predict & (df[regression_label]<thres)].mean()\n",
    "comparison[\"suspicious_positive\"] = series_bad\n",
    " \n",
    "comparison"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:cee-hacks-ml]",
   "language": "python",
   "name": "conda-env-cee-hacks-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
